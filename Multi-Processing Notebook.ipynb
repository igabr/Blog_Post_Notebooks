{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blog Post: Multi-Processing\n",
    "\n",
    "This notebook will introduce and walk you through some aspects of the Multi-Processing library.\n",
    "\n",
    "This particular library allows us to write parallelizable code that will speed up the execution of our programs and allow us to obtain results rapidly.\n",
    "\n",
    "### What do I mean by parallelizable?\n",
    "\n",
    "Let's say that you had 17 URL's that you wanted to scrape. Without using the multi-processing library, you would probably scrape each URL in a _serial_ manner. By this, I mean that you would scrape one URL at a time, waiting for the result of your scraping function to return a result, before proceding to the next URL.\n",
    "\n",
    "Now, this can be quite time consuming, depending on the information you are trying to extract. Wouldnt it be better if you could scrape all 17 links at the same time?\n",
    "\n",
    "This is precisely what the multi-processing library lets you do - you can see this in action by looking at one of my previous projects: https://tinyurl.com/yd4juxjz\n",
    "\n",
    "By writing parallel code, you can utilise all the cores in your machine! A single python interpreter is using only 1 core to do all computations. This is why python code runs in a _serial_ manner. This is also related to pythons well known [GIL](https://wiki.python.org/moin/GlobalInterpreterLock)\n",
    "\n",
    "For those of you who think this is too good to be true, I'll give you a short example that will show code running in parallel before taking a deeper dive into the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from multiprocessing import Pool, cpu_count\n",
    "import time\n",
    "\n",
    "cpu_count() # I have 8 cores on my computer - I could use all of them over time by writing parallelizable code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = ([\"Ibrahim\", 5], [\"Jennifer\", 2], [\"Juan\", 1], [\"Andrew\", 3], [\"Anna\", 7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_data(arg):\n",
    "    print(f'Process {arg[0]} starting and will complete in {arg[1]} seconds')\n",
    "    time.sleep(arg[1])\n",
    "    print(f'Process {arg[0]} is now finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def execute_pool():\n",
    "    p = Pool(2)  # Run 2 processes at a time.\n",
    "    p.map(process_data, data)\n",
    "    p.terminate() # terminte all processes in the pool\n",
    "    p.join() # exit the pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process Ibrahim starting and will complete in 5 seconds\n",
      "Process Jennifer starting and will complete in 2 seconds\n",
      "Process Jennifer is now finished!\n",
      "Process Juan starting and will complete in 1 seconds\n",
      "Process Juan is now finished!\n",
      "Process Andrew starting and will complete in 3 seconds\n",
      "Process Ibrahim is now finished!\n",
      "Process Anna starting and will complete in 7 seconds\n",
      "Process Andrew is now finished!\n",
      "Process Anna is now finished!\n"
     ]
    }
   ],
   "source": [
    "execute_pool()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above print out might look like sequential output - but it isnt! Be sure to run the above code yourself. Even play with the number of processes and see how it changes the results!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Class\n",
    "\n",
    "Let's now introduce the Process class. This class allows your to manually create processes tht you can then execute in parallel.\n",
    "\n",
    "Most importantly, when using the process class, the function you want to run in parallel is called the target. Furthermore, you must provide the arguments of the target function.\n",
    "\n",
    "Let's look at an example to make this clear:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def say_my_name(name):\n",
    "    print(f'My name is {name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names = ['Ibrahim', 'Juan', 'Andrew', 'Jennifer', 'Mary', \"Elizabeth\"]\n",
    "to_process = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for name in names:\n",
    "    proc = Process(target=say_my_name, args=(name,)) # You must have the trailing comma\n",
    "    to_process.append(proc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above, we see that the `args` keyword argument is used to specify the input to the `say_my_name` function.\n",
    "\n",
    "Now, if you want multiple argument to be passed to the function - it is better to slightly alter the say_my_name function to expect a tuple. You can then break out the tuple internally and run your operations.\n",
    "\n",
    "It is very common to write functions that are specifically designed to be executed in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Process(Process-3, initial)>,\n",
       " <Process(Process-4, initial)>,\n",
       " <Process(Process-5, initial)>,\n",
       " <Process(Process-6, initial)>,\n",
       " <Process(Process-7, initial)>,\n",
       " <Process(Process-8, initial)>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we can see that we have a list of processes - the length of this list corresponding the number of elements in the `names` list!\n",
    "\n",
    "Now, in order to execute these processes, we need to iterate over this process list, start the process. We then need to exit the completed processes.\n",
    "\n",
    "It is important to note here, that even though we are iterating through the `to_process` list in a sequential manner via a `for` loop. The results do **not** need to be in that same order.\n",
    "\n",
    "The order of the results will correspond to which process was completed first!\n",
    "\n",
    "In this example, the results will most likely be in the same order as the iteration of the `for` loop as we are running the incredibly simple function `say_my_name`.\n",
    "\n",
    "Later on, I will show you how to ensure order while exploiting parallelism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My name is Ibrahim\n",
      "My name is Juan\n",
      "My name is Andrew\n",
      "My name is Jennifer\n",
      "My name is Mary\n",
      "My name is Elizabeth\n"
     ]
    }
   ],
   "source": [
    "# .start() executes a process\n",
    "\n",
    "for p in to_process:\n",
    "    p.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code block shows that the processes were executed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Process(Process-3, stopped)>,\n",
       " <Process(Process-4, stopped)>,\n",
       " <Process(Process-5, stopped)>,\n",
       " <Process(Process-6, stopped)>,\n",
       " <Process(Process-7, stopped)>,\n",
       " <Process(Process-8, stopped)>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above shows how the processes in our list are now in a stopped state. This means they have executed.\n",
    "\n",
    "It is a best practice to formally terminate all processes so that they are no lingering in the background and eating up resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# .join() terminates and exits a process safely.\n",
    "\n",
    "for p in to_process:\n",
    "    p.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Queue Class\n",
    "\n",
    "Sometimes, we wish to share data across different processes. This can get messy with processes interfering with each other and results getting jumbled.\n",
    "\n",
    "However, the built in Queue class will allow you to store data in a FIFO structure that can easily be passed between processes.\n",
    "\n",
    "Let's do a simple example below where we store some generated passwords via processes and store the results in a queue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "from multiprocessing import Queue\n",
    "\n",
    "queue = Queue()\n",
    "\n",
    "def create_password(length):\n",
    "    cipher = ''.join(random.SystemRandom().choice(string.ascii_uppercase + string.digits) for _ in range(length))\n",
    "    queue.put(cipher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "to_process = [Process(target=create_password, args=(i,)) for i in range(5,13)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Process(Process-9, initial)>,\n",
       " <Process(Process-10, initial)>,\n",
       " <Process(Process-11, initial)>,\n",
       " <Process(Process-12, initial)>,\n",
       " <Process(Process-13, initial)>,\n",
       " <Process(Process-14, initial)>,\n",
       " <Process(Process-15, initial)>,\n",
       " <Process(Process-16, initial)>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for p in to_process:\n",
    "    p.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, we see no output! This is because we do not have a call to the `print` function in `create_password`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for p in to_process:\n",
    "    p.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so how do we get the results of our processes? They are in the queue!\n",
    "\n",
    "In order to get the results, which are in FIFO order, we need to use the `.get()` method.\n",
    "\n",
    "This needs to be done for **every** result in the queue. I like to use a list comprehension to extract everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output = [queue.get() for _ in to_process]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D4OU2',\n",
       " 'DB9C1H',\n",
       " '6VFZHYQ',\n",
       " '5VYFZHAI',\n",
       " 'VBWVEHV5L',\n",
       " 'FX520HRLHD',\n",
       " 'I0J1VL9D2GW',\n",
       " 'Z0BL6J6VHSQ9']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have our resultsin the order that they were **successfully** processed in. Again - this is not guarnateed to be the same order as the iteration of the `for` loop used to create the processes.\n",
    "\n",
    "To further illustrate why the `Queue` class is useful, let me show you another example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_queue(data, q):\n",
    "    \n",
    "    for item in data:\n",
    "        print(f'Placing {item} in queue')\n",
    "        q.put(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_queue(data, q):\n",
    "    for _ in data:\n",
    "        data = q.get()\n",
    "        print(f'currently processing {data}')\n",
    "        processed = sigmoid(data)\n",
    "        print(f'Result for {data} is {processed}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code block below, notice how we will have to processes accessing the queue at the same time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Placing 0.1 in queue\n",
      "Placing 0.2 in queue\n",
      "Placing 0.30000000000000004 in queue\n",
      "currently processing 0.1\n",
      "Placing 0.4 in queue\n",
      "Result for 0.1 is 0.52497918747894\n",
      "currently processing 0.2\n",
      "Placing 0.5 in queue\n",
      "Placing 0.6 in queue\n",
      "Result for 0.2 is 0.549833997312478\n",
      "currently processing 0.30000000000000004\n",
      "Placing 0.7000000000000001 in queue\n",
      "Placing 0.8 in queue\n",
      "Result for 0.30000000000000004 is 0.574442516811659\n",
      "currently processing 0.4\n",
      "Placing 0.9 in queue\n",
      "Result for 0.4 is 0.598687660112452\n",
      "currently processing 0.5\n",
      "Result for 0.5 is 0.6224593312018546\n",
      "currently processing 0.6\n",
      "Result for 0.6 is 0.6456563062257954\n",
      "currently processing 0.7000000000000001\n",
      "Result for 0.7000000000000001 is 0.6681877721681662\n",
      "currently processing 0.8\n",
      "Result for 0.8 is 0.6899744811276125\n",
      "currently processing 0.9\n",
      "Result for 0.9 is 0.7109495026250039\n"
     ]
    }
   ],
   "source": [
    "q = Queue()\n",
    "data = np.arange(0.1, 1, 0.1)\n",
    "process_1 = Process(target=load_queue, args=(data, q))\n",
    "process_2 = Process(target=process_queue, args=(data, q))\n",
    "process_1.start()\n",
    "process_2.start()\n",
    "\n",
    "q.close() # Indicate that no more data will be put on this queue by the current process. The background thread will quit once it has flushed all buffered data to the pipe.\n",
    "q.join_thread() # ensures that all data in the buffer has been flushed to the pipe.\n",
    "\n",
    "process_1.join()\n",
    "process_2.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome, from the above, we can really see the power of multi-processing. The use of the queue class allows our processes to work in harmony!\n",
    "\n",
    "### Pool Class\n",
    "\n",
    "The Pool class will allow you to maintain order in your results. Furthermore, the Pool class will allow you to use the following methods:\n",
    "\n",
    "- `map` -> just like the built-in `map` function\n",
    "\n",
    "- `apply` -> just like the built-in `apply` function\n",
    "\n",
    "- `map_async` -> results will be returned in the order processed.\n",
    "\n",
    "- `apply_async` -> results will be returned in the order processed/\n",
    "\n",
    "Note that async means asynchronous.\n",
    "\n",
    "Let's just jump into how to use these with examples!\n",
    "\n",
    "A pool, is exactly what is sounds like. Imagine a swimming pool filled with processes. Each process in the pool will need to be executed in parallel with all other processes.\n",
    "\n",
    "Thanks to `map_async` and `apply_async` we can mimic the beahviour of the `Process` class - that is, unordered outputs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.52497918747894001,\n",
       " 0.54983399731247795,\n",
       " 0.57444251681165903,\n",
       " 0.598687660112452,\n",
       " 0.62245933120185459,\n",
       " 0.6456563062257954,\n",
       " 0.66818777216816616,\n",
       " 0.6899744811276125,\n",
       " 0.71094950262500389]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply example\n",
    "\n",
    "pool = Pool(processes=3) # 3 processes to run in parallel at a time\n",
    "output = [pool.apply(sigmoid, args=(i,)) for i in np.arange(0.1,1,0.1)]\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.52497918747894001,\n",
       " 0.54983399731247795,\n",
       " 0.57444251681165903,\n",
       " 0.598687660112452,\n",
       " 0.62245933120185459,\n",
       " 0.6456563062257954,\n",
       " 0.66818777216816616,\n",
       " 0.6899744811276125,\n",
       " 0.71094950262500389]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# map example - be sure to check the beginning of notebook for another example.\n",
    "\n",
    "output2 = pool.map(sigmoid, np.arange(0.1,1,0.1))\n",
    "output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.52497918747894001,\n",
       " 0.54983399731247795,\n",
       " 0.57444251681165903,\n",
       " 0.598687660112452,\n",
       " 0.62245933120185459,\n",
       " 0.6456563062257954,\n",
       " 0.66818777216816616,\n",
       " 0.6899744811276125,\n",
       " 0.71094950262500389]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# map async example\n",
    "\n",
    "output3 = pool.map_async(sigmoid, np.arange(0.1,1,0.1))\n",
    "output3.get() #Notice how the .get() method returns ALL results here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.52497918747894001,\n",
       " 0.54983399731247795,\n",
       " 0.57444251681165903,\n",
       " 0.598687660112452,\n",
       " 0.62245933120185459,\n",
       " 0.6456563062257954,\n",
       " 0.66818777216816616,\n",
       " 0.6899744811276125,\n",
       " 0.71094950262500389]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply async example\n",
    "\n",
    "apply_objects = [pool.apply_async(sigmoid, args=(i, )) for i in np.arange(0.1,1,0.1)]\n",
    "output4 = [result.get() for result in apply_objects] # Notice that .get() method applied to every element here\n",
    "output4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brilliant - you have now seen all the cases within the `Pool` class. The above example is simple, however, it illustrates the necessary methods - feel free to do your own benchmarking tests.\n",
    "\n",
    "Be creative with the code setup. Remember that for multiple arguments to be passed, it is often better to modify the target function to accept a tuple or input parameters. Another way is to use a tuple of list with `Pool.map` to pass function arguments!\n",
    "\n",
    "Also be sure to read out the amazing [documentation](https://docs.python.org/3.6/library/multiprocessing.html) for this library - I only just scrateched the surface with this notebook!\n",
    "\n",
    "As always - feel free to contact me: igabr@uchicago.edu or [@Gabr\\_Ibrahim](https://twitter.com/Gabr_Ibrahim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
